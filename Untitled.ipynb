{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Modules\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom Modules\n",
    "from load_data import *\n",
    "from image_preprocessing import extract_image_features, load_image\n",
    "from pretrained_transformers import get_danish_transformers, get_english_transformers\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "lang = 'english'\n",
    "image_path = 'data/Flickr8k_Dataset/'\n",
    "captions_file = get_captions_file(lang)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_captions(captions_file)\n",
    "df = sample_from_df(df, seed=seed)\n",
    "\n",
    "# Extracting captions and image names from the dataframe\n",
    "all_captions = extract_captions(df)\n",
    "all_image_names = extract_image_names(df, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizer & BertModel\n",
    "if lang == 'danish':\n",
    "    tokenizer, BertModel = get_danish_transformers()\n",
    "elif lang =='english':\n",
    "    tokenizer, BertModel = get_english_transformers()\n",
    "else:\n",
    "    print(\"Wrong language specified: \", lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(all_captions,\n",
    "                      padding=True,\n",
    "                      return_tensors=\"tf\",\n",
    "                      return_token_type_ids=False,\n",
    "                      return_attention_mask=False)['input_ids']\n",
    "\n",
    "# Splits\n",
    "train_pct = int(0.8*len(all_image_names))\n",
    "test_pct = int(0.9*len(all_image_names))\n",
    "img_names_train, img_names_val, img_names_test = all_image_names[\n",
    "    :train_pct], all_image_names[train_pct:test_pct], all_image_names[test_pct:]\n",
    "cap_train, cap_val, cap_test = input_ids[:train_pct], input_ids[\n",
    "    train_pct:test_pct], input_ids[test_pct:]\n",
    "\n",
    "assert (len(img_names_train) == len(cap_train)) & (len(\n",
    "    img_names_test) == len(cap_test)) & (len(img_names_val) == len(cap_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "train_dataset = data_to_tensors(img_names_train, cap_train)\n",
    "val_dataset = data_to_tensors(img_names_val, cap_val)\n",
    "test_dataset = data_to_tensors(img_names_test, cap_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "vocab_size = tokenizer.vocab_size #should perhaps be fixed size and not entire vocab?\n",
    "num_layer = 4\n",
    "d_model = 768\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "row_size = 8\n",
    "col_size = 8\n",
    "target_vocab_size = vocab_size + 1\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "transformer = Transformer(num_layer, d_model, num_heads, dff, row_size, col_size,\n",
    "                          target_vocab_size, BertModel, max_pos_encoding=target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "#transformer.compile(optimizer=optimizer, loss=train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing\n",
    "checkpoint_path = \"./checkpoints\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "@tf.function\n",
    "def train_step(img_tensor, tar):\n",
    "    tar_inp = tar[:, :-1]  # cuts last column off\n",
    "    tar_real = tar[:, 1:]  # cuts first column off\n",
    "    dec_mask = create_masks_decoder(tar_inp) # creates a tensor mask of (tar_inp.shape[0], 1, tar_inp.shape[1], tar_inp.shape[1])\n",
    "    # E.g. 8 (tar_inp.shape[0] -> batch size) instances of a list of a 15x15 matrice\n",
    "    with tf.GradientTape() as tape:  # backprop.GradientTape\n",
    "        predictions, _ = transformer(\n",
    "            inp=img_tensor, tar=tar_inp, training=True, look_ahead_mask=dec_mask)\n",
    "\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = []\n",
    "epoch_loss_list = []\n",
    "epoch_accuracy_list = []\n",
    "batch_list = []\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # (img_tensor: image, tar: input sentence)\n",
    "    for (batch, (img_tensor, tar)) in enumerate(train_dataset):\n",
    "        train_step(img_tensor, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "            batch_list.append(batch)\n",
    "            loss_list.append(train_loss.result())\n",
    "            accuracy_list.append(train_accuracy.result())\n",
    "    epochs_list.append(epoch)\n",
    "    epoch_loss_list.append(train_loss.result())\n",
    "    epoch_accuracy_list.append(train_accuracy.result())\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                        train_loss.result(),\n",
    "                                                        train_accuracy.result()))\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
    "\n",
    "transformer.save_weights(\"./saved_models/ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_list, loss_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Per Batch')\n",
    "plt.savefig('./plots/bert_english_training_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_list, accuracy_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Per Batch')\n",
    "plt.savefig('./plots/bert_english_training_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_list, loss_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Per Batch')\n",
    "plt.savefig('./plots/bert_english_training_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_list, accuracy_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Per Batch')\n",
    "plt.savefig('./plots/bert_english_training_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    start_token = tokenizer.cls_token_id\n",
    "    end_token = tokenizer.sep_token_id\n",
    "    decoder_input = [start_token]\n",
    "    output = tf.expand_dims(decoder_input, 0) #tokens\n",
    "    result = [] #word list\n",
    "\n",
    "    for i in range(100):\n",
    "        dec_mask = create_masks_decoder(output)\n",
    "        predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        if predicted_id == end_token:\n",
    "            return result,tf.squeeze(output, axis=0), attention_weights\n",
    "        result.append(tokenizer.convert_ids_to_tokens(int(predicted_id)))\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return result,tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_preprocessing import load_inception_v3\n",
    "image_features_extract_model = load_inception_v3()\n",
    "predicted_captions = []\n",
    "for i, image in enumerate(img_names_val[:5]):\n",
    "    caption, result, attention_weights = evaluate(image)\n",
    "    predicted_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_captions = []\n",
    "for rid in range(0, len(cap_val)):\n",
    "    real_captions.append(' '.join([tokenizer.ids_to_tokens[i] for i in cap_val[rid].numpy() if i not in [0]])[6:-6].split())\n",
    "print(len(real_captions), real_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption in captions:\n",
    "    for i in caption:\n",
    "        if i==\"<unk>\":\n",
    "            caption.remove(i)\n",
    "for caption in real_captions:\n",
    "    for i in caption:\n",
    "        if i==\"<unk>\":\n",
    "            caption.remove(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_join = ' '.join(caption)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = caption"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
