{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-andrew",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-lecture",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-reproduction",
   "metadata": {},
   "source": [
    "    First section here devoted to getting the data. We will be working with the COCO and Flickr8k dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr Data\n",
    "flickr_images_path = \"data/Flickr8k_Dataset/\"\n",
    "flickr_annotations = \"data/Flickr8k_text/Flickr8k.token.txt\"\n",
    "jpgs = glob.glob(flickr_images_path + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO train2014 caption annotation files\n",
    "annotation_folder = 'data/coco_annotations_train2014/'\n",
    "if not os.path.exists(annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                        cache_subdir=os.path.abspath('data/'),\n",
    "                        origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                        extract = True)\n",
    "    coco_annotations = os.path.dirname(annotation_zip)+'/coco_annotations_train2014/captions_train2014.json'\n",
    "    os.remove(annotation_zip)\n",
    "else:\n",
    "    coco_annotations = 'data/coco_annotations_train2014/captions_train2014.json'\n",
    "\n",
    "# Download COCO images train2014 image files\n",
    "# The dataset contains over 82,000 images, each of which has at least 5 different caption annotations.\n",
    "\n",
    "image_folder = 'data/coco_images_train2014/'\n",
    "if not os.path.exists(image_folder):\n",
    "    image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                    cache_subdir=os.path.abspath('data/'),\n",
    "                    origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                    extract = True)\n",
    "    coco_images_path = os.path.dirname(image_zip) + image_folder\n",
    "    os.remove(image_zip)\n",
    "else:\n",
    "    coco_images_path = image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-hungarian",
   "metadata": {},
   "source": [
    "    Now to choose which dataset to work with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Flickr run this cell\n",
    "annotations = flickr_annotations\n",
    "image_path = flickr_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For COCO run this cell\n",
    "annotations = coco_annotations\n",
    "image_path = coco_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_annotations(file):\n",
    "    \n",
    "    text = file.read()\n",
    "    datatxt = []\n",
    "    for line in text.split('\\n'):\n",
    "        col = line.split('\\t')\n",
    "        if len(col) == 1:\n",
    "            continue\n",
    "        w = col[0].split(\"#\")\n",
    "        datatxt.append(w + [col[1].lower()])\n",
    "    \n",
    "    print(\"Number of annotations = {}\".format(len(datatxt)))\n",
    "    print(\"Number of images = {}\".format(len(jpgs)))\n",
    "    \n",
    "    return datatxt\n",
    "\n",
    "def load_json_annotations(file):\n",
    "    \n",
    "    annotations = json.load(file)\n",
    "    #print(\"Type: \", type(annotations), \", Keys: \", list(annotations.keys()))\n",
    "    #print()\n",
    "    print(\"Info: \", annotations['info'])\n",
    "    print()\n",
    "    print(\"Number of annotations = {}\".format(len(annotations['annotations'])))\n",
    "    print(\"Number of images = {}\".format(len(annotations['images'])))\n",
    "    \n",
    "    datatxt = []\n",
    "    for annotation in annotations['annotations']:\n",
    "        id_ = annotation['id']\n",
    "        image_id = 'COCO_train2014_' + '%012d.jpg' % (annotation['image_id'])\n",
    "        caption = annotation['caption']\n",
    "        datatxt.append([image_id, id_, caption])\n",
    "    \n",
    "    return datatxt\n",
    "\n",
    "def annotations_to_df(annotation_file):\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        file_name, file_ext = os.path.splitext(file.name)\n",
    "        if file_ext.endswith('.txt'):\n",
    "            datatxt = load_txt_annotations(file)\n",
    "        elif file_ext.endswith('.json'):\n",
    "            datatxt = load_json_annotations(file)\n",
    "        else:\n",
    "            print(\"Wrong file type.\")      \n",
    "    data = pd.DataFrame(datatxt,columns=[\"image_name\", \"index\", \"caption\"])\n",
    "    data = data.reindex(columns =['index','image_name','caption'])\n",
    "    data = data[data.image_name != '2258277193_586949ec62.jpg.1']\n",
    "    \n",
    "    return data\n",
    "    \n",
    "data = annotations_to_df(annotations)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-progress",
   "metadata": {},
   "source": [
    "    A small look at the images and their captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_filenames = np.unique(data.image_name.values)\n",
    "\n",
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "count = 1\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for jpgfnm in uni_filenames[10:12]:\n",
    "    filename = image_path + jpgfnm\n",
    "    captions = list(data[\"caption\"].loc[data[\"image_name\"]==jpgfnm].values)\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "\n",
    "    ax = fig.add_subplot(npic,2,count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0,i,caption,fontsize=14)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-treasury",
   "metadata": {},
   "source": [
    "## Sample of the Data\n",
    "    I sample x number of rows from the dataframe for experimentation. This will make it faster to test the subsequent steps. \n",
    "    Set sample to false, when wanting to train on the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = False\n",
    "num_examples = 64\n",
    "if sample:\n",
    "    data_shuffle = data.sample(frac=1)\n",
    "    df = data_shuffle[:num_examples].copy()\n",
    "else:\n",
    "    df = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-onion",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for caption in df.caption.values:\n",
    "    vocabulary.extend(caption.split())\n",
    "print('Vocabulary Size: %d' % len(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text_original):\n",
    "    text_no_punctuation = text_original.translate(string.punctuation)\n",
    "    \n",
    "    return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "    text_len_more_than1 = \"\"\n",
    "    for word in text.split():\n",
    "        if len(word) > 1:\n",
    "            text_len_more_than1 += \" \" + word\n",
    "    \n",
    "    return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "    text_no_numeric = \"\"\n",
    "    for word in text.split():\n",
    "        isalpha = word.isalpha()\n",
    "        if isalpha:\n",
    "            text_no_numeric += \" \" + word\n",
    "    \n",
    "    return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "    text = remove_punctuation(text_original)\n",
    "    text = remove_single_character(text)\n",
    "    text = remove_numeric(text)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "for i, caption in enumerate(df.caption.values):\n",
    "    newcaption = text_clean(caption)\n",
    "    df[\"caption\"].iloc[i] = newcaption\n",
    "    \n",
    "clean_vocabulary = []\n",
    "for caption in df.caption.values:\n",
    "    clean_vocabulary.extend(caption.split())\n",
    "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-telephone",
   "metadata": {},
   "source": [
    "    Next, we save all the captions and image paths in two lists so that we can load the images at once using the path set. We also add ‘<start>’ and ‘<end>’ tags to every caption so that the model understands the starting and end of each caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for caption  in df[\"caption\"].astype(str):\n",
    "    caption = '<start> ' + caption + ' <end>'\n",
    "    all_captions.append(caption)\n",
    "\n",
    "all_image_names = []\n",
    "for image_name in df[\"image_name\"]:\n",
    "    full_image_path = image_path + image_name\n",
    "    all_image_names.append(full_image_path)\n",
    "\n",
    "print(all_captions[0])\n",
    "print(all_image_names[0])\n",
    "print(f\"len(all_image_names) : {len(all_image_names)}\")\n",
    "print(f\"len(all_captions) : {len(all_captions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-thumbnail",
   "metadata": {},
   "source": [
    "    The following is only needed if we're running on the entire dataset and want the number of samples to be a multiple of \n",
    "    64. Say 40000, if batch size = 64, i.e. 625 batches. To do this we define a function to limit the dataset to 40000 images and captions. Set limit to True if you wish to define a limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_limiter(num, all_captions, all_image_names):\n",
    "    all_captions_shuffled, image_names_shuffled = shuffle(all_captions, all_image_names, random_state=1)\n",
    "    train_captions = all_captions_shuffled[:num]\n",
    "    image_names = image_names_shuffled[:num]\n",
    "    \n",
    "    return train_captions, image_names\n",
    "\n",
    "limit = False\n",
    "limit = 40000\n",
    "\n",
    "if limit:\n",
    "    captions, image_names = data_limiter(limit, all_captions, all_image_names)\n",
    "else:\n",
    "    captions = all_captions\n",
    "    image_names = all_image_names\n",
    "\n",
    "print(len(all_captions), len(captions), len(all_image_names), len(image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captions[5], image_names[5])\n",
    "\n",
    "npic = 5\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "image_load = load_img(image_names[5], target_size=target_size)\n",
    "cap = list(df[\"caption\"].loc[df[\"image_name\"]==os.path.basename(image_names[5])].values)\n",
    "ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
    "ax.imshow(image_load)\n",
    "plt.show()\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-anniversary",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "### Image Features Using InceptionV3\n",
    "    We use V3 to extract the image features from each image, hence we don't include any layers after the last conv. neural layer. The V3 model expects the images to be of size 299 x 299, so we resize them to that. The output of the last conv. layer in the model, i.e. the image features, will be 8x8x2048. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each Keras Application expects a specific kind of input preprocessing. \n",
    "#For InceptionV3, call tf.keras.applications.inception_v3.preprocess_input on your inputs before passing them to the model.\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "#include_top = whether to include the fully-connected layer at the top, as the last layer of the network. Default to True.\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "# After the last conv. layer in the V3 model the shape of the output is 8x8x2048. \n",
    "# Rest has not been included.\n",
    "\n",
    "\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-visibility",
   "metadata": {},
   "source": [
    "    Next, let’s Map each image name to the function to load the image. We will pre-process each image with InceptionV3 and cache the output to disk and image features are reshaped to 64×2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train = sorted(set(image_names))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "#image_dataset is an iterable of images, reads images here\n",
    "\n",
    "image_dataset = image_dataset.map(load_image, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64) #batch(multi-element) image_dataset transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of input images: \", image_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-castle",
   "metadata": {},
   "source": [
    "    We extract the features and store them in the respective .npy files and then pass those features through the encoder.\n",
    "    NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, path in tqdm(image_dataset):\n",
    "    #img shape: (16, 299, 299, 3) which the V3 model expects. Format: (batch, img shape)\n",
    "    batch_features = image_features_extract_model(img) #shape: (16, 8, 8, 2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3])) #shape: (16, 64, 2048)\n",
    "    #batch_features.shape[0], -1 = (16, (8*8*2048)) = (16, 131072)\n",
    "    #(16, 131072, batch_features[3]) = (16, 131072/2048) = (16, 64, 2048)\n",
    "    \n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-comedy",
   "metadata": {},
   "source": [
    "    Next, we tokenize the captions and build a vocabulary of all the unique words in the data. We will also limit the vocabulary size to the top 5000 words to save memory. We will replace words not in vocabulary with the token < unk >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-waters",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 500 # Change this if necessary\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                 oov_token=\"<unk>\",\n",
    "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "tokenizer.fit_on_texts(captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(captions)\n",
    "train_seqs_padded = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-colony",
   "metadata": {},
   "source": [
    "    Next, Create training and validation sets using an 80-20 split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(image_names, train_seqs_padded, test_size=0.2, random_state=0)\n",
    "print(len(img_name_train), len(img_name_val), len(cap_train), len(cap_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-longitude",
   "metadata": {},
   "source": [
    "    Next, let’s create a tf.data dataset to use for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-expense",
   "metadata": {},
   "source": [
    "### Bert Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "systematic-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forward-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "responsible-avatar",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(768,), dtype=float32, numpy=\n",
       "array([ 1.36302961e-02, -2.64904164e-02, -2.35031322e-02, -7.78762111e-03,\n",
       "        8.58919881e-03, -7.66453659e-03, -9.88080073e-03,  6.01844303e-03,\n",
       "        4.69211303e-03, -3.09841223e-02,  1.88834984e-02, -6.00926392e-03,\n",
       "       -1.66518576e-02,  1.16844112e-02, -3.62453945e-02,  8.34820606e-03,\n",
       "       -1.21121341e-03,  1.03216469e-02,  1.66918822e-02, -3.03537678e-02,\n",
       "       -1.23718306e-02, -2.51732711e-02, -8.96016415e-03,  8.19942355e-03,\n",
       "       -2.00110544e-02, -1.59005877e-02, -3.83943762e-03,  1.42410689e-03,\n",
       "        7.04999268e-03,  1.60923810e-03, -2.77643139e-03,  9.49308742e-03,\n",
       "       -2.27681026e-02,  1.93166081e-02, -1.34418225e-02, -2.37626694e-02,\n",
       "       -1.46171171e-02,  9.77347139e-03, -2.24284781e-03,  3.06421630e-02,\n",
       "        6.78290473e-03, -2.64705042e-03, -1.85534023e-02, -1.23629477e-02,\n",
       "        7.64887501e-03, -2.54611461e-03, -3.14984620e-01,  6.37608860e-03,\n",
       "        4.89142798e-02, -7.76357949e-03,  6.09193854e-02,  2.13458557e-02,\n",
       "       -3.97413969e-02,  2.28525415e-01,  2.65018120e-02, -1.01439247e-03,\n",
       "       -7.84803275e-03, -1.99948763e-03,  1.70566440e-02, -3.32703851e-02,\n",
       "        4.54208069e-03,  6.17510220e-03, -1.00766785e-01, -2.09728573e-02,\n",
       "       -1.45117534e-04, -9.66566801e-03,  1.08705126e-02, -1.47855869e-02,\n",
       "        2.64372793e-04,  2.11662222e-02,  1.64920092e-02, -5.19281393e-03,\n",
       "       -1.18574845e-02, -9.91590694e-03, -1.43631073e-02, -1.24046300e-02,\n",
       "       -1.29728364e-02,  2.67776158e-02, -1.09862396e-02,  1.05721261e-02,\n",
       "       -2.55658124e-02,  5.24936849e-03,  1.58900600e-02, -5.15039125e-03,\n",
       "       -7.58589944e-03,  2.02591065e-02, -7.01553235e-03,  1.63586587e-02,\n",
       "        1.74871646e-02,  5.42973354e-03, -8.64030980e-03,  2.88209580e-02,\n",
       "       -7.89639819e-03,  1.92590039e-02,  2.38680635e-02, -4.34724474e-03,\n",
       "        5.56619279e-02, -2.19399612e-02,  4.17785766e-03, -5.72157511e-03,\n",
       "        2.67120283e-02, -5.03707118e-03,  2.49226037e-02, -1.34287644e-02,\n",
       "       -8.43372289e-03,  9.81877372e-02, -1.29400101e-03,  1.28650768e-02,\n",
       "       -1.59295532e-03,  3.64371738e-03,  1.55689074e-02,  1.86197367e-02,\n",
       "       -9.06429905e-03, -1.97404847e-02,  1.05304709e-02, -2.73585669e-03,\n",
       "       -7.52828876e-03,  1.14921085e-03,  2.61621829e-03, -6.27566548e-03,\n",
       "       -8.60958733e-03,  6.62212193e-01, -3.22350394e-03, -4.13085595e-02,\n",
       "        3.30468966e-03, -2.50398088e-03,  1.28384185e-04, -6.80731609e-03,\n",
       "        6.02914998e-03, -9.84684378e-03,  8.06414615e-03, -1.98147539e-03,\n",
       "        2.58009955e-02,  5.74285677e-03, -1.07122920e-02,  2.91757267e-02,\n",
       "        5.94136678e-03,  2.47952007e-02, -1.78865120e-02,  7.31829703e-01,\n",
       "        1.09637966e-02,  5.99423703e-03, -4.61568795e-02,  4.01308127e-02,\n",
       "       -9.74813011e-03, -8.94964576e-01,  1.63845997e-02, -1.98156573e-03,\n",
       "        1.46910148e-02, -1.98368113e-02, -1.76106673e-02, -4.52629116e-04,\n",
       "       -1.86048485e-02, -1.56596899e-02, -1.07089039e-02,  1.80157498e-02,\n",
       "       -3.41488793e-03, -1.26323486e-02,  4.28770809e-03, -3.91686410e-01,\n",
       "        1.00159254e-02, -1.09546240e-02,  4.51325299e-03, -5.11501450e-03,\n",
       "        4.99681523e-03,  1.78518314e-02,  1.13128368e-02,  2.65186070e-03,\n",
       "        3.36578488e-01, -1.81684569e-02,  1.31701836e-02,  7.39272498e-03,\n",
       "        5.25205676e-03, -9.62302648e-03,  1.28437355e-02,  4.15541619e-01,\n",
       "       -9.72473994e-03, -4.24385071e-03,  5.52871148e-04,  1.82714723e-02,\n",
       "       -1.38893293e-03, -2.05018581e-03, -8.19461327e-03, -6.59789157e-06,\n",
       "       -7.27639534e-04, -1.46250043e-03, -6.98716054e-03, -6.96334057e-03,\n",
       "       -8.07005353e-03,  1.99357904e-02,  4.83699981e-03,  8.68827198e-03,\n",
       "       -4.92460988e-02, -2.00280305e-02,  1.41244510e-03,  1.04441326e-02,\n",
       "       -1.12364292e-02, -4.46537230e-03, -2.04911046e-02, -2.76536345e-02,\n",
       "       -3.70793976e-02,  1.32153165e-02,  6.94981664e-02, -3.11085265e-02,\n",
       "        7.05620972e-03,  1.08866133e-02, -7.80895725e-03, -1.05012851e-02,\n",
       "       -4.87347739e-03, -6.83988095e-04,  1.47171682e-02,  4.43422003e-03,\n",
       "        1.60123222e-02, -1.04272384e-02, -2.57665403e-02, -2.26991564e-01,\n",
       "        8.65687653e-02,  2.34532356e-02,  4.63618115e-02,  3.56088160e-03,\n",
       "        2.13529356e-02,  2.37030610e-02, -2.02521384e-02,  2.15802323e-02,\n",
       "        7.26519339e-03,  2.09328398e-01,  1.21076088e-02,  1.08690113e-02,\n",
       "        7.05682999e-03, -3.11315525e-02,  2.05047149e-02,  3.22482735e-03,\n",
       "       -2.27244454e-03,  5.53417904e-03,  3.05627892e-03,  1.95416324e-02,\n",
       "        1.28272909e-03,  1.59524828e-02, -1.54584860e-02, -3.84551310e-03,\n",
       "       -4.94174380e-03, -1.04456339e-02,  7.05159223e-03,  2.24672444e-03,\n",
       "       -9.36426688e-03,  1.91628467e-02,  1.42390663e-02, -1.58163495e-02,\n",
       "        8.74130335e-03,  2.47374009e-02, -7.37768784e-03, -4.09746431e-02,\n",
       "        9.49477591e-03,  1.47000654e-02,  2.68191192e-02,  1.07058184e-02,\n",
       "        1.06208464e-02, -7.18162069e-03, -8.54020659e-03,  1.22611681e-02,\n",
       "       -4.86791972e-03, -9.61363688e-03,  7.87646277e-04,  3.85039710e-02,\n",
       "       -7.74846599e-03, -6.50177849e-03,  3.43519612e-03,  2.29312485e-04,\n",
       "        5.74564980e-03, -4.84412909e-03, -9.08981357e-03,  8.62983428e-03,\n",
       "        5.47401840e-03,  2.22743098e-02, -2.12175958e-02, -2.67945062e-02,\n",
       "       -3.53372749e-03,  1.07848821e-02,  1.24752810e-02, -6.11601165e-03,\n",
       "        1.07292002e-02, -9.79549531e-03,  1.85429435e-02, -6.04875525e-03,\n",
       "       -4.57436871e-03,  2.70894775e-03,  1.56320836e-02, -1.29284747e-02,\n",
       "       -3.07780295e-03, -1.03247780e-02, -7.95497932e-03, -6.30648434e-02,\n",
       "        2.10615154e-02, -6.67171227e-03,  8.46162438e-03,  1.44750802e-02,\n",
       "        1.14768982e-01, -2.28377860e-02, -3.74909341e-02, -3.62184420e-02,\n",
       "       -3.19942571e-02, -8.92515667e-03,  3.17202881e-02, -1.12597151e-02,\n",
       "       -1.29797012e-01, -1.03149912e-03, -4.72423434e-03, -2.00922396e-02,\n",
       "       -9.45209503e-01, -2.21776236e-02, -4.42967284e-04,  1.97105445e-02,\n",
       "        3.34024541e-02, -1.05133392e-02,  1.44921327e-02, -1.96972825e-02,\n",
       "       -9.84518789e-03, -1.73465703e-02,  2.34716050e-02,  7.65700489e-02,\n",
       "        1.95036661e-02,  9.36165266e-03,  8.26718938e-03, -1.04705961e-02,\n",
       "       -1.99319818e-03,  2.00004000e-02,  2.04851069e-02,  1.09774182e-02,\n",
       "        1.77197158e-02,  1.35315536e-02,  7.36822654e-03,  3.49062117e-04,\n",
       "        1.87724142e-03,  1.99764296e-02, -3.20410840e-02, -8.91688187e-03,\n",
       "        1.28996996e-02, -1.33314515e-02,  6.62069023e-03, -5.70627721e-03,\n",
       "       -1.14824949e-02,  8.39072559e-03, -6.41617319e-03,  1.58160143e-02,\n",
       "        7.89214205e-03,  4.41766996e-03,  2.25678515e-02,  1.02394046e-02,\n",
       "       -3.01944936e-04,  1.32940561e-02, -2.16060802e-02,  3.88319278e-03,\n",
       "        2.44749114e-02,  4.38083857e-02, -2.10305746e-03, -1.21627217e-02,\n",
       "       -4.07863185e-02,  1.55648440e-02,  1.47500960e-02,  1.66445728e-02,\n",
       "        2.80825850e-02,  1.89195003e-03, -1.47327810e-04, -2.62076817e-02,\n",
       "        2.37795878e-02,  1.86565725e-04, -2.29308987e-03,  3.03340564e-03,\n",
       "       -1.72940381e-02, -2.30010860e-02,  8.60039983e-03, -3.34974863e-02,\n",
       "        2.56595314e-02, -1.92246698e-02, -2.71856114e-02, -2.10197251e-02,\n",
       "       -3.52125652e-02, -1.82279572e-03, -8.28402955e-03,  1.12124179e-02,\n",
       "        1.03868572e-02, -3.41935039e-01, -1.97049882e-03,  1.15582468e-02,\n",
       "        5.19756135e-03,  7.44978013e-03,  5.71415620e-03,  2.84012165e-02,\n",
       "       -7.75506487e-03,  1.06818285e-02, -1.26571711e-02, -1.80645268e-02,\n",
       "        2.66807829e-03,  3.39472480e-03, -4.55645360e-02, -2.11703014e-02,\n",
       "       -1.78302396e-02,  3.46786855e-03, -2.20507737e-02, -5.41759469e-03,\n",
       "       -1.15165235e-02, -3.41548808e-02, -3.03346361e-03, -1.39152100e-02,\n",
       "        6.21729763e-03, -1.11005018e-02, -1.53075717e-02,  9.21878964e-03,\n",
       "       -7.56647578e-03,  6.56846631e-03,  8.09349585e-03,  3.11389077e-03,\n",
       "       -5.50467940e-03, -3.13471258e-02,  2.21400671e-02,  1.08646145e-02,\n",
       "       -2.78493315e-02, -4.95798467e-03,  1.88040570e-03,  1.00066170e-01,\n",
       "       -1.80127460e-03, -4.87924740e-03,  1.55341132e-02, -2.01786347e-02,\n",
       "       -1.23505816e-02, -1.38710234e-02,  1.14390440e-02, -9.02081002e-03,\n",
       "        1.25804460e-02, -2.59730965e-02, -2.03977972e-02, -1.94639934e-03,\n",
       "        4.31894092e-03,  2.07071025e-02,  5.00290003e-03, -1.06785288e-02,\n",
       "        1.22983297e-02,  1.02686351e-02,  2.22276766e-02,  2.97542252e-02,\n",
       "       -2.63924920e-03,  1.92855261e-02, -1.51369954e-02,  2.19137922e-01,\n",
       "        1.30299442e-02, -7.44597614e-03, -9.68179666e-04,  2.97358520e-02,\n",
       "        9.87224560e-03, -5.66876819e-03,  4.25175857e-03,  1.89407561e-02,\n",
       "       -6.39092084e-03,  8.05899594e-03, -6.78929966e-03,  6.08776882e-03,\n",
       "       -5.39700920e-03,  7.57761765e-04,  1.13737443e-03, -5.00352168e-03,\n",
       "       -1.61590765e-03,  1.67642701e-02,  9.12514143e-03,  1.30198440e-02,\n",
       "       -1.03683127e-02,  2.21405085e-02, -2.54106545e-03, -1.52270934e-02,\n",
       "        2.34435741e-02,  8.40764318e-04, -1.14654787e-01,  2.70171417e-03,\n",
       "       -4.49613016e-03,  2.97619583e-04, -3.96116404e-03,  8.90380761e-05,\n",
       "        2.86832061e-02,  5.00683440e-03,  1.65094007e-02,  7.89830636e-04,\n",
       "        5.77283977e-03,  3.26846838e-02, -1.04572982e-01,  1.29894102e-02,\n",
       "        1.12775201e-02,  1.19431019e-02,  1.52584529e-02, -6.24113192e-04,\n",
       "        1.06818290e-04,  1.20866196e-02,  7.29839830e-03,  2.77579296e-02,\n",
       "        1.75719596e-02, -6.03447435e-03,  1.72111504e-02,  1.41205266e-02,\n",
       "        6.46627173e-02,  9.18125734e-03,  3.25547555e-03, -3.26666646e-02,\n",
       "        2.91320533e-02, -1.77704953e-02,  1.53020618e-03, -2.99437456e-02,\n",
       "       -2.07058545e-02, -3.65278451e-03, -1.54966665e-02,  1.52233923e-02,\n",
       "       -1.47508672e-02, -2.23813597e-02,  6.96359808e-03, -8.08383245e-03,\n",
       "       -2.45834514e-03, -2.06767991e-02,  8.81323684e-03, -6.95537543e-04,\n",
       "        1.69653930e-02,  1.85351849e-01,  3.58428253e-04,  1.08123943e-02,\n",
       "       -4.23910795e-03,  8.17790534e-03,  3.41438539e-02, -1.89956231e-03,\n",
       "        2.99390801e-03,  3.68984038e-04, -1.01440484e-02, -5.74157387e-03,\n",
       "       -5.76755498e-03,  1.75653026e-01, -1.57931866e-03, -2.66170744e-02,\n",
       "       -1.25720920e-02,  3.04214947e-04, -1.21322703e-02, -1.41684860e-02,\n",
       "        1.21541684e-02,  8.47002305e-03, -1.62841566e-02,  2.69828271e-03,\n",
       "       -6.85535092e-03,  2.78290689e-01,  2.40604673e-02,  1.11303618e-02,\n",
       "        7.60954688e-04,  3.13406408e-01,  2.16681920e-02,  1.02772499e-02,\n",
       "       -3.00650299e-02, -8.35654978e-03,  5.24879713e-03, -1.12868864e-02,\n",
       "       -1.82659663e-02,  1.18141398e-02,  1.26620783e-02,  2.90359982e-04,\n",
       "        7.02543417e-04, -1.40838241e-02,  1.29246823e-02,  3.95044452e-03,\n",
       "       -7.95681216e-03,  3.27940471e-02,  7.38393376e-03,  2.46090591e-02,\n",
       "        9.61089507e-03, -8.72064568e-03,  9.25714616e-03, -3.58496397e-03,\n",
       "       -8.99956655e-03,  2.31204252e-03, -1.84745789e-02, -1.96103901e-02,\n",
       "        1.19939791e-02,  6.71555893e-03,  1.99032351e-02,  3.07028051e-02,\n",
       "       -4.95379511e-03, -6.16730750e-02, -6.49862830e-03, -2.13169176e-02,\n",
       "       -3.36498185e-03,  2.32001487e-03, -6.22244226e-03,  3.74575099e-03,\n",
       "        1.15419412e-02, -1.01812147e-02, -8.47108196e-03,  1.16025833e-02,\n",
       "       -5.62465563e-03, -1.02195656e-02, -8.65009381e-04, -1.22850332e-02,\n",
       "       -8.74870922e-03, -1.12649687e-02,  1.63224600e-02,  1.51604228e-02,\n",
       "        1.88819449e-02,  5.15570119e-03, -8.86157528e-03,  4.21530800e-03,\n",
       "       -1.94496866e-02, -8.73646419e-03, -9.78667662e-03,  1.16668837e-02,\n",
       "        5.06128045e-03,  2.82206433e-03, -7.17945537e-03,  9.33059026e-03,\n",
       "       -4.96633165e-02,  1.77082699e-02, -2.09590886e-02, -3.39889340e-02,\n",
       "        2.25813501e-03,  5.17482031e-03, -1.01328425e-01,  2.10518320e-03,\n",
       "        5.56435389e-03,  1.36074983e-03,  8.83875135e-03,  1.02436515e-02,\n",
       "       -3.80718103e-03,  5.92089351e-03,  6.79928763e-03,  1.15943560e-02,\n",
       "       -1.18024452e-02, -2.42333720e-03, -5.15040243e-03, -1.19032627e-02,\n",
       "        1.40749961e-02, -4.07010550e-03, -2.94649545e-02, -1.75793108e-03,\n",
       "        4.36544931e-03,  1.04286196e-02,  3.70961688e-02,  8.64933245e-03,\n",
       "        1.58708394e-02,  1.80343445e-02, -3.21654021e-03, -2.19405834e-02,\n",
       "        2.62740664e-02, -7.69413589e-03, -5.96178044e-03, -1.41787576e-02,\n",
       "        8.02808255e-03,  1.12932706e-02, -6.69359724e-05,  1.28987459e-02,\n",
       "        1.00564705e-02, -6.39194041e-04,  2.02985127e-02,  3.15278396e-03,\n",
       "       -4.89877304e-03,  3.27537348e-03, -1.10032171e-01,  1.84139479e-02,\n",
       "        2.22721160e-03, -2.21850704e-02, -4.86715138e-03,  1.96431810e-03,\n",
       "        3.09281647e-02, -8.95994343e-03, -1.14457896e-02, -1.37944613e-02,\n",
       "        7.19425781e-03, -5.89654874e-03,  2.26053759e-03, -2.61137113e-02,\n",
       "       -5.66162961e-03,  6.50734594e-03,  9.22186747e-02, -6.72433199e-03,\n",
       "        4.44270263e-04,  7.28463242e-03, -1.10208420e-02,  7.88017001e-04,\n",
       "       -3.88781144e-03,  1.04891406e-02,  9.28825047e-03,  1.88951287e-02,\n",
       "        2.18077283e-02,  6.25902845e-04, -2.65189297e-02,  7.03434169e-04,\n",
       "       -2.90665384e-02, -9.15145408e-03,  1.04180002e-03,  8.32218025e-03,\n",
       "       -8.75475630e-03, -2.06370605e-03, -1.14498613e-02, -8.89854331e-04,\n",
       "       -4.40619234e-03,  2.36291923e-02, -2.72212513e-02,  3.20084654e-02,\n",
       "        6.63254317e-03, -1.13020167e-02, -1.01383473e-03, -1.69024408e-01,\n",
       "       -8.44734907e-03,  2.85363067e-02,  1.41168491e-03, -1.21357571e-02,\n",
       "       -1.47805559e-02,  4.99602268e-03,  3.39163989e-02,  5.27099799e-03,\n",
       "        1.73823498e-02, -4.63150162e-03,  1.16796857e-02, -9.13949404e-03,\n",
       "        1.83098875e-02,  1.23209078e-02, -2.48707049e-02,  1.15353093e-02,\n",
       "        5.03082527e-03,  5.50280977e-03, -7.21836183e-03, -5.52100036e-03,\n",
       "        1.70849822e-02,  5.72358957e-03,  1.74627290e-03,  1.99689786e-03,\n",
       "        6.16695872e-03,  2.93465774e-03,  1.39464969e-02, -1.99836283e-03,\n",
       "        1.00907190e-02,  1.03877496e-03, -6.19020127e-03,  3.09046172e-02,\n",
       "        6.60380861e-03, -9.12226960e-02, -1.84113588e-02,  5.41854091e-03,\n",
       "        2.43961867e-02,  1.56962927e-02, -1.27421310e-02,  1.81257818e-02,\n",
       "       -2.61377450e-02,  1.11702802e-02, -1.30580664e-02, -1.93859488e-02,\n",
       "       -5.98284276e-03,  1.91756245e-02,  1.99615164e-03, -2.15384597e-03,\n",
       "        3.30033526e-02,  1.84074119e-02, -5.94982086e-03, -3.25329788e-03,\n",
       "       -1.89171489e-02, -1.58974566e-02, -4.70567215e-03,  5.41616837e-03,\n",
       "       -3.00370529e-02,  8.67734477e-03, -1.79420249e-03,  6.68255007e-03,\n",
       "       -1.19292494e-02, -1.40759647e-02,  1.67088956e-02,  1.68595754e-03,\n",
       "       -3.38415336e-03,  8.68047308e-03,  7.13398913e-03,  1.51473405e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.weights[0][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(['this is the first sentence', 'another setence', 'hello there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"Hello\"))\n",
    "tokenizer.ids_to_tokens[101], tokenizer.ids_to_tokens[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "transparent-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', add_special_tokens=False)\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "all_captions = ['this is the first sentence', 'another setence', 'hello there']\n",
    "token_ids = tokenizer(all_captions,\n",
    "                  padding=True,\n",
    "                  return_tensors=\"tf\")['input_ids']\n",
    "\n",
    "output = model(**token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-oklahoma",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-freeware",
   "metadata": {},
   "source": [
    "    The positional encoding uses sine and cosine functions of different frequencies. For every odd index on the input vector, create a vector using the cos function, and for every even index, create a vector using the sin function.\n",
    "\n",
    "    Then add those vectors to their corresponding input embeddings which successfully gives the network information on the position of each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding_1d(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis, :],\n",
    "                           d_model)\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def positional_encoding_2d(row,col,d_model):\n",
    "    assert d_model % 2 == 0\n",
    "    row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
    "    col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
    "\n",
    "    angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "    angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
    "\n",
    "    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
    "    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
    "    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
    "    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
    "    pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-horizon",
   "metadata": {},
   "source": [
    "## Multi-Head Attentionn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-vegetarian",
   "metadata": {},
   "source": [
    "    Calculate the attention weights. q, k, v must have matching leading dimensions. k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v. The mask has different shapes depending on its type (padding or look ahead) but it must be broadcastable for addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) #seq: (batch x seq_len) 0 where not 0 and 1 where 0.\n",
    "    seq = seq[:, tf.newaxis, tf.newaxis, :]\n",
    "    return seq  # (batch_size, 1, 1, seq_len) -> a list of 8 lists tf.Tensor([[[[seq_len]]] ... [[[seq_len]]]]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) #creates lower triangular matrix of (size x size)\n",
    "                                                                #mask = 1 - lower triangular matrix becomes\n",
    "                                                                # an upper triangular matrix\n",
    "    return mask  # (seq_len, seq_len) or (size, size)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9) \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                 (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "            tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-sister",
   "metadata": {},
   "source": [
    "## Encode-Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, row_size,col_size,rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n",
    "        self.pos_encoding = positional_encoding_2d(row_size,col_size,self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                         for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                            look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-florence",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size,\n",
    "              target_vocab_size,max_pos_encoding, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                          target_vocab_size,max_pos_encoding, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training,look_ahead_mask=None,dec_padding_mask=None,enc_padding_mask=None):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-johns",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_layer = 4\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "row_size = 8\n",
    "col_size = 8\n",
    "target_vocab_size = top_k + 1\n",
    "dropout_rate = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                    epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "transformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,\n",
    "                          target_vocab_size, max_pos_encoding=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-paintball",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks_decoder(tar):\n",
    "    #target is batch x max_sent_len-1 matrice, e.g. 8x15\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) #returns the maximum of the two tensors\n",
    "    \n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, tar):\n",
    "    tar_inp = tar[:, :-1] #cuts last column off\n",
    "    tar_real = tar[:, 1:] #cuts first column off\n",
    "    dec_mask = create_masks_decoder(tar_inp) #creates a tensor mask of (tar_inp.shape[0], 1, tar_inp.shape[1], tar_inp.shape[1])\n",
    "                                            #E.g. 8(tar_inp.shape[0] -> batch size) instances of a list of a 15x15 matrice \n",
    "    with tf.GradientTape() as tape: #backprop.GradientTape\n",
    "        \n",
    "        predictions, _ = transformer(inp=img_tensor, tar=tar_inp, training=True, look_ahead_mask=dec_mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)   \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = []\n",
    "batch_list = []\n",
    "loss_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    for (batch, (img_tensor, tar)) in enumerate(dataset): #(img_tensor: image, tar: input sentence)\n",
    "        train_step(img_tensor, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            epochs_list.append(epoch)\n",
    "            batch_list.append(batch)\n",
    "            loss_list.append(train_loss.result())\n",
    "            accuracy_list.append(train_accuracy.result())\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                               train_loss.result(),\n",
    "                                               train_accuracy.result()))\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list, batch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-wilson",
   "metadata": {},
   "source": [
    "# BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    start_token = tokenizer.word_index['<start>']\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "    decoder_input = [start_token]\n",
    "    output = tf.expand_dims(decoder_input, 0) #tokens\n",
    "    result = [] #word list\n",
    "\n",
    "    for i in range(100):\n",
    "        dec_mask = create_masks_decoder(output)\n",
    "        predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        if predicted_id == end_token:\n",
    "            return result,tf.squeeze(output, axis=0), attention_weights\n",
    "        result.append(tokenizer.index_word[int(predicted_id)])\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return result,tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "caption, result, attention_weights = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "for i in caption:\n",
    "    if i==\"<unk>\":\n",
    "        caption.remove(i)\n",
    "\n",
    "for i in real_caption:\n",
    "    if i==\"<unk>\":\n",
    "        real_caption.remove(i)\n",
    "\n",
    "result_join = ' '.join(caption)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = caption\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
    "print(f\"BLEU-1 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
    "print(f\"BLEU-2 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
    "print(f\"BLEU-3 score: {score*100}\")\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
    "print(f\"BLEU-4 score: {score*100}\")\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Predicted Caption:', ' '.join(caption))\n",
    "temp_image = np.array(Image.open(image))\n",
    "plt.imshow(temp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-thursday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
